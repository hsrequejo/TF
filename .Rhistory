load(rData_AllGnorm_path_list[[1]])
allGnorm = G_norm_mean
for (i in 2:length(rData_AllGnorm_path_list)) {
load(rData_AllGnorm_path_list[[i]])
allGnorm = append(allGnorm, G_norm_mean, after = length(allGnorm))
allGnorm
}
G_norm_dristrib_plot(allGnorm)
save(allGnorm, file ="./RDatas_Gnorm_distrib_grupo/rdml_2_100_30_n10")
# Faz a unificação de varias distribuicoes de Gnorm
library(fs)
library(ggplot2)
# informa o R que o diretório do documento atual 'e o diretorio de trabalho
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Aux_functions.R", encoding="utf-8")
# Gera uma lista com os caminhos dos RDatas do Gnorm
rData_AllGnorm_path_list <- dir_ls("./RDatas_2_100_30", glob = "*.RData")
rData_AllGnorm_path_list
load(rData_AllGnorm_path_list[[1]])
allGnorm = G_norm_mean
for (i in 2:length(rData_AllGnorm_path_list)) {
load(rData_AllGnorm_path_list[[i]])
allGnorm = append(allGnorm, G_norm_mean, after = length(allGnorm))
allGnorm
}
G_norm_dristrib_plot(allGnorm)
save(allGnorm, file ="./RDatas_Gnorm_distrib_grupo/rdml_2_100_30_n10")
# Faz a unificação de varias distribuicoes de Gnorm
library(fs)
library(ggplot2)
# informa o R que o diretório do documento atual 'e o diretorio de trabalho
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Aux_functions.R", encoding="utf-8")
# Gera uma lista com os caminhos dos RDatas do Gnorm
rData_AllGnorm_path_list <- dir_ls("./RDatas_2_100_50", glob = "*.RData")
rData_AllGnorm_path_list
load(rData_AllGnorm_path_list[[1]])
allGnorm = G_norm_mean
for (i in 2:length(rData_AllGnorm_path_list)) {
load(rData_AllGnorm_path_list[[i]])
allGnorm = append(allGnorm, G_norm_mean, after = length(allGnorm))
allGnorm
}
G_norm_dristrib_plot(allGnorm)
save(allGnorm, file ="./RDatas_Gnorm_distrib_grupo/rdml_2_100_50_n10")
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Centr_Mono.R', encoding = 'UTF-8', echo=TRUE)
CentrCompare = function(rData_centr_path = "./RDatas_AllCentr/ants_allCentr.RData", ranking_cutoff = 10){
load(rData_centr_path)
eig = eig_formated
deg = deg_formated
# 1=clo  2=btw   3=eig   4=deg   5=Gnorm
centr_list = list(clo, btw, eig, deg, Gnorm)
#separa os nohs mais centrais
most_central_list = list()
for (i in 1:length(centr_list)) {
centr_temp = centr_list[[i]]
centr_temp = sort(centr_temp, decreasing = TRUE)
centr_temp = centr_temp[1:ranking_cutoff]
most_central_list[[i]] = centr_temp
}
#compara quantos nos encontrados no Gnorm estao presentes nos outros metodos
Gnorm_most_central = most_central_list[[5]]
similarity_bin = rep(0, length(most_central_list))
names(similarity_bin) = c("clo", "btw", "eig", "deg", "Gnorm")
similarity_string_list = list()
for (i in 1:(length(most_central_list))) {
list_temp = list()
for (j in 1:ranking_cutoff) {
for (k in 1:ranking_cutoff) {
if (names(Gnorm_most_central[j]) == names(most_central_list[[i]][k])) {
similarity_bin[i] = similarity_bin[i] + 1
list_temp = append(list_temp, names(Gnorm_most_central[j]))
}
}
}
similarity_string_list[[i]] = list_temp
}
similarity_bin = similarity_bin/ranking_cutoff
#compara a distancia entre os rankings encontrados no Gnorm com os que estao presentes nos outros metodos
Gnorm_most_central = most_central_list[[5]]
similarity_dist = rep(0, length(most_central_list))
names(similarity_dist) = c("clo", "btw", "eig", "deg", "Gnorm")
for (i in 1:(length(most_central_list))) {
list_temp = list()
for (j in 1:ranking_cutoff) {
for (k in 1:ranking_cutoff) {
if (names(Gnorm_most_central[j]) == names(most_central_list[[i]][k])) {
similarity_dist[i] = similarity_dist[i] + (1/(1+abs(j-k)))
}
}
}
}
similarity_dist = similarity_dist/ranking_cutoff
similarity_dist
most_central_list
results_list = list()
results_list[[1]] = similarity_bin
results_list[[2]] = similarity_dist
results_list[[3]] = most_central_list
return(results_list)
}
# informa o R que o diretório do documento atual 'e o diretorio de trabalho
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Gera uma lista com os caminhos dos RDatas do Gnorm
rData_AllCentr_path_list <- dir_ls("./RDatas_AllCentr", glob = "*.RData")
simil_bin = data.frame(clo=numeric(),
btw=numeric(),
eig=numeric(),
deg=numeric(),
Gnorm=numeric())
simil_dist = data.frame(clo=numeric(),
btw=numeric(),
eig=numeric(),
deg=numeric(),
Gnorm=numeric())
for (i in 1:length(rData_AllCentr_path_list)) {
centr_comp = CentrCompare(rData_centr_path = rData_AllCentr_path_list[[i]], ranking_cutoff = 10)
simil_bin = rbind(simil_bin, centr_comp[[1]])
simil_dist = rbind(simil_bin, centr_comp[[2]])
}
colnames(simil_bin) = c("clo", "btw", "eig", "deg", "Gnorm")
colnames(simil_dist) = c("clo", "btw", "eig", "deg", "Gnorm")
simil_dist
simil_bin
# Gera redes aleatorias dados o numero de camadas, conexoes e nos (em construcao)
# Cria uma rede aleatória osando o numero de camadas e numero de conexoes, numero de nos e
# quantas redes aleatorias devem ser geradas e um sufixo para o nome do arquivo caso necessario
Create_random_ML = function(number_of_layers, number_of_conections, number_of_nodes, sufix = ""){
nodes_ID = seq(from = 1, to = number_of_nodes, by = 1)
for (i in 1:number_of_nodes) {
nodes_ID[i] = paste("node", nodes_ID[i], sep="")
}
links_df = data.frame(from = numeric(0), to = numeric(0), layer = numeric(0))
pair_temp = sample(sample(nodes_ID, 2, replace = FALSE))
pair_temp = sort(pair_temp)
links_df[1,1] = pair_temp[1]
links_df[1,2] = pair_temp[2]
links_df[1,3] = sample(1:number_of_layers, 1)
times = 0
layers_rand = sample(1:number_of_layers, number_of_layers, replace=FALSE)
layers_rand
layer_choosen = layers_rand[1]
i = 1
while (i < number_of_conections) {
times = 0
pair_temp = sample(nodes_ID, 2, replace = FALSE)
sort(pair_temp)
layers_rand = sample(1:number_of_layers, number_of_layers, replace=FALSE)
layer_choosen = layers_rand[1]
#checks if the pair already exists more than X times in the links_df
for (j in 1:length(links_df[,1])) {
if (pair_temp[1] == links_df[j,1] && pair_temp[2] == links_df[j,2]) {
times = times + 1
}
}
if (times < number_of_layers) {
links_df[i,1] = pair_temp[1]
links_df[i,2] = pair_temp[2]
links_df[i,3] = layer_choosen
i = i+1
}
}
links_df
# minimizes dual effect (not sure if it removes the dual link problem completely)
for (i in 1:length(links_df[,1])) {
for (j in 1:length(links_df[,1])) {
if (links_df[i,1] == links_df[j,1] && links_df[i,2] == links_df[j,2] && links_df[i,3] == links_df[j,3] && i != j) {
while(links_df[i,3] == links_df[j,3]){
layers_rand = sample(1:number_of_layers, number_of_layers, replace=FALSE)
links_df[i,3] = layers_rand[1]
}
}
}
}
#checks for multiple links on the same layer
number_of_duals = 0
duals = data.frame(indexa = numeric(0), indexb = numeric(0), from = numeric(0), to = numeric(0), layer = numeric(0))
k = 1
for (i in 1:length(links_df[,1])) {
for (j in 1:length(links_df[,1])) {
if (links_df[i,1] == links_df[j,1] && links_df[i,2] == links_df[j,2] && links_df[i,3] == links_df[j,3] && i != j) {
number_of_duals = number_of_duals + 1
duals[k,1] = i
duals[k,2] = j
duals[k,3] = links_df[i,1]
duals[k,4] = links_df[i,2]
duals[k,5] = links_df[i,3]
k = k + 1
}
}
}
#removes dual links entirely
links_df = links_df[!seq_len(nrow(links_df)) %in% duals[,1], ]
#extract the nodes list
nodes_raw = links_df[,1]
nodes_raw = append(nodes_raw, links_df[,2])
nodes_raw
nodes_df = unique(nodes_raw)
nodes_df = sort(nodes_df)
nodes_df = cbind(nodes_df, rep("filler", length(nodes_df)))
colnames(nodes_df) = c("name", "enchimento")
nodes_df
links_df
b = Sys.time()
paste0(round(as.numeric(difftime(time1 = b, time2 = a, units = "secs")), 3), " Seconds")
#gera uma string com o nome do arquivo csv da seguinte forma: camadas_conexoes_nos
file_name_links = paste("rand_ml_",number_of_layers, "_", number_of_conections, "_", number_of_nodes, "_links", sufix,".csv", sep ="")
file_name_nodes = paste("rand_ml_",number_of_layers, "_", number_of_conections, "_", number_of_nodes, "_nodes", sufix,".csv", sep ="")
#creates a csv file for the nodes_df and links_df using the file name on the directory of this R script
write.csv(links_df, file_name_links, row.names = FALSE, quote = FALSE)
write.csv(nodes_df, file_name_nodes, row.names = FALSE, quote = FALSE)
}
#Create_random_ML(3, 100, 20)
layers = c(2, 3, 5)
conections = c(250)
nodes_proportion = c(0.2, 0.3, 0.5)
number_of_duplicate_networks = 10
total_networks_to_be_generated = length(layers)*length(conections)*length(nodes_proportion)*number_of_duplicate_networks
n = 1
for (i in 1:length(layers)) {
for (j in 1:length(conections)) {
for (k in 1:length(nodes_proportion)) {
nodes = round(nodes_proportion[k]*conections[j],0)
for (count in 1:number_of_duplicate_networks) {
Create_random_ML(layers[i], conections[j], nodes, sufix = count)
cat(paste0(round( n / total_networks_to_be_generated * 100, 2), '%  '))
n = n+1
}
}
}
}
###################################################################################################################
#Faz a analise de Gnorm de todas as redes (nodes.csv e links.csv) de um diretório salvando um RData para cada uma)#
###################################################################################################################
library(fs)
library(stringr)
library(multinet)
library(dplyr)
library(plyr)
library(igraph)
# informa o R que o diretório de trabalho é o do documento atual
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Aux_functions.R", encoding="utf-8")
###Em construcao####
#G_Analisys em forma de funcao
G_Analisys = function(nodes_path, links_path, network_name, partitions_of_omega = 10, min_gamma, max_gamma, gamma_min = 0.25, gamma_max = 4, gamma_spacing = 0.25, iterations = 100){
# Entradas. Nao esquecer de alterar o net_name, bib_ref e file_to_save, pois serao usadas no RMarkdown----
nodes = read.csv(nodes_path, header=T, as.is=T)
links = read.csv(links_path, header=T, as.is=T)
# Pra ser usado no corpo do texto do documento R Markdown
net_name = network_name
#Caminho onde deve ser salvo o .RData. Lembrar de mudar o nome para nao sobreescrever o antigo
file_to_save = paste("./RDatas_Gnorm/", net_name, ".RData", sep="")
#----
#ordena os nos. Importante para referenciar os nos corretamente
nodes = nodes[order(nodes$name),]
# Converte nodes e links em objeto multinet para an?lise e Igraph para visualizacao usando uma funcao auxilar
net_multinet = Convert_to_Multinet(nodes, links)
#salva um dataframe com as propriedades da rede
propriedades_rede = Net_prop(net_multinet)
#calcula um layout (igraph) e plota o grafo usando a funcao aux CustomPlot----
links_no_dupl = links[-which(duplicated(links[,c("from", "to")])==T),] # retira os duplicados para nao influenciar no layout
net_layout = graph_from_data_frame(d = links_no_dupl, vertices = nodes, directed = F) #usado somente para calcular o layout
layout = layout_nicely(net_layout) # igraph
#----
#Gera o banco de dados 'seq_G_Merged' e o vetor de omegas vec_W para um numero dado de particoes de omega----
partitions_of_omega = partitions_of_omega # numero de particoes entre zero e 1 para omega
seq_G = Create_seq_G_Merged(net_multinet, partitions_of_omega)
vec_W = Create_vec_W(partitions_of_omega)
#----
# variaveis para inicializar o vetor de gammas
gamma_min = gamma_min
gamma_max = gamma_max
gamma_spacing = gamma_spacing
gammas = seq(from = gamma_min, to = gamma_max, by = gamma_spacing)
Seq_G_Mean_gamma_list = list() #guarda os diferentes datasets de Seq_G_Mean
G_norm_ordered_list = list() #guarda os diferentes nohs selecionados para plot
G_norm_list = list()
cont_perc = 1 # usado apenas para mostrar a porcentagem de conclusao
progression = winProgressBar(title = "Progress bar", min = 0,max = iterations*length(gammas) , width = 300)
for (gamma_index in 1:length(gammas)) {
#gerar uma sequencia de banco de dados seqG----
seq_G_list = list()
iterations = iterations
for (i in 1:iterations) {
seq_G_list[[i]] = Create_seq_G_Merged(net_multinet, partitions_of_omega, gamma = gammas[gamma_index])
#cat(cont_perc*100/(iterations*length(gammas)), "%  ")
setWinProgressBar(progression, cont_perc, title=paste(round(cont_perc*100/(iterations*length(gammas)), digits = 2),"% done  - ", net_name))
cont_perc = cont_perc + 1
}
#----
#Remove os nomes da primeira coluna do SeqG list. Tendo problemas para somar deviso a essas strings
seq_G_list_no_names = list()
for (i in 1:length(seq_G_list)) {
seq_G_list_temp = seq_G_list[[i]]
seq_G_list_temp[,1] = 1
seq_G_list_no_names[[i]] = seq_G_list_temp
}
#soma todos os dataframes em seq_G_list em um df----
seq_G_sum = seq_G_list_no_names[[1]]
for (i in 2:length(seq_G_list)) {
seq_G_sum = seq_G_sum + seq_G_list_no_names[[i]]
}
seq_G_sum
#----
#calcula a media de seq_G_sum
seq_G_mean = seq_G_sum / iterations
#insere o nome dos nohs novamente no df de media. Este valor se perde no caminho devido aos calculos
seq_G_mean[,1] = seq_G_list[[1]]$actor
#calcula o desvio padrao usando a funcao auxiliar 'StdDev_list_of_seq_G'
seq_G_StdDev = StdDev_list_of_seq_G(seq_G_list)
#vetor ordenado de acordo com o indice G do df mean----
nodes_G_norm = Sort_Nodes_by_Total_G(seq_G_mean, ordered = FALSE)
nodes_G_norm_Ordered = Sort_Nodes_by_Total_G(seq_G_mean, ordered = TRUE)
#----
#Guarda o Seq_G_mean e nodes_G_norm em uma lista de acordo com o gamma usado
Seq_G_Mean_gamma_list[[gamma_index]] = cbind(seq_G_mean, gammas[gamma_index])
G_norm_list[[gamma_index]] = nodes_G_norm
}
#dataframe equivalente ao Seg_G_Mean, mas agora considerando gamma, usando funcao auxiliar
seq_Gnorm_gamma_mean = Unite_list_of_dataframes(Seq_G_Mean_gamma_list)
#enconta os valores medios de G levendo em conta a variacao de gamma
G_norm_sum = G_norm_list[[1]]
for (i in 2:length(G_norm_list)) {
G_norm_sum = G_norm_sum + G_norm_list[[i]]
}
#----
#calcula a media de seq_G_sum
G_norm_mean = G_norm_sum / (length(G_norm_list))
G_norm_mean
#ordena G_norm_mean
G_norm_mean_ordered =  sort(G_norm_mean, decreasing = TRUE)
#Salva as as variaveis que são usadas no R Markdown em um arquivo .RData
save(gammas, vec_W, iterations, partitions_of_omega, links, nodes, layout, Seq_G_Mean_gamma_list,
seq_Gnorm_gamma_mean, G_norm_mean, G_norm_mean_ordered, net_name, file = file_to_save)
}
#gera uma lista com o nome (string) de todos os arquivos csv de um diretório
path <- "./Input"
list_files <- dir_ls(path, glob = "*.csv")
#Separa uma lista para os arquivos 'links' e para o arquivo 'nodes'. Eh necessario que alguma palavra dentro do nome do arquivo .csv seja 'links' ou 'nodes'
list_nodes_path = list()
list_links_path = list()
for (i in 1:length(list_files)) {
if (grepl("nodes",list_files[i])) {
list_nodes_path[[length(list_nodes_path)+1]] = list_files[i]
} else if(grepl("links",list_files[i])){
list_links_path[[length(list_links_path)+1]] = list_files[i]
}
}
list_nodes_path
list_links_path
net_names_list = str_sub(list_nodes_path, 9)
net_names_list = str_sub(net_names_list, 1, -5)
net_names_list
#Executa a analise de Gnorm para cada uma das redes da lista e salva um RData com o nome da rede
for (i in 1:length(net_names_list)) {
G_Analisys(list_nodes_path[[i]], list_links_path[[i]], network_name = net_names_list[i], iterations = 10)
}
# Rascunho: Agrega uma rede multicamada e analisa a centralidade monocamada da rede agregada
#install.packages("CINNA")
library(fs)
library(stringr)
library(multinet)
library(igraph)
library(dplyr)
library(CINNA)
library(plyr)
library(igraph)
#Funcoes auxiliares-------------------------------------------------
source("Aux_functions.R", encoding="utf-8")
#-------------------------------------------------------------------
# calcula as centralidades de uma rede monocamada
AllCentr = function(nodes_path = "./Network_Inputs/bat-plant_nodes.csv", links_path = "./Network_Inputs/bat-plant_links.csv",
rData_Gnorm_path = "./RDatas/bats.RData", rData_to_save_name = "bats_allCentr.RData"){
nodes = read.csv(nodes_path, header=T, as.is=T)
links = read.csv(links_path, header=T, as.is=T)
#ordena os nos. Importante para referenciar os nos corretamente
nodes = nodes[order(nodes$name),]
# Converte nodes e links em objeto multinet para an?lise e Igraph para visualizacao usando uma funcao auxilar
net_multinet = Convert_to_Multinet(nodes, links)
# Agrega a rede multicamada em uma rede monocamada com e sem conexoes duplicadas
links_no_dupl = links[-which(duplicated(links[,c("from", "to")])==T),]
net_mono_nodup = graph_from_data_frame(d = links_no_dupl, vertices = nodes, directed = F)
net_mono = graph_from_data_frame(d = links, vertices = nodes, directed = F)
#TODO:Analisar a centralidade da rede agregada
clo = closeness(net_mono, normalized = FALSE)
btw = betweenness(net_mono, directed = FALSE, normalized = TRUE)
eig = eigen_centrality(net_mono)
eig_formated = eig$vector
deg = centr_degree(net_mono)
deg_formated = deg$res
names(deg_formated) = names(clo)
#part = part_coeff(net_mono)
# Faz a leitura do .Rdata que contém o G_norm dos nohs do script G_Analysis
load(rData_Gnorm_path)
Gnorm = G_norm_mean
save(clo, btw, eig_formated, deg_formated, Gnorm, file = rData_to_save_name)
}
# informa o R que o diretório de trabalho é o do documento atual
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#gera uma lista com o nome (string) de todos os arquivos csv de um diretório
path <- "./Input"
list_files <- dir_ls(path, glob = "*.csv")
#Separa uma lista para os arquivos 'links' e para o arquivo 'nodes'. Eh necessario que alguma palavra dentro do nome do arquivo .csv seja 'links' ou 'nodes'
list_nodes_path = list()
list_links_path = list()
for (i in 1:length(list_files)) {
if (grepl("nodes",list_files[i])) {
list_nodes_path[[length(list_nodes_path)+1]] = list_files[i]
} else if(grepl("links",list_files[i])){
list_links_path[[length(list_links_path)+1]] = list_files[i]
}
}
list_nodes_path
list_links_path
net_names_list = str_sub(list_nodes_path, 9)
net_names_list = str_sub(net_names_list, 1, -5)
net_names_list
# Gera uma lista com os caminhos dos RDatas do Gnorm
rData_Gnorm_path_list <- dir_ls("./RDatas_Gnorm", glob = "*.RData")
progression = winProgressBar(title = "Progress bar", min = 0,max = length(net_names_list) , width = 300)
for (i in 1:length(net_names_list)) {
AllCentr(nodes_path = list_nodes_path[[i]], links_path = list_links_path[[i]],
rData_Gnorm_path = rData_Gnorm_path_list[[i]],
rData_to_save_name = paste("./RDatas_AllCentr/", net_names_list[[i]], "_allCentr.RData", sep = ""))
setWinProgressBar(progression, i, title=paste(round(i*100/length(net_names_list) , digits = 2),"% done  - ", net_name))
}
# Faz a unificação de varias distribuicoes de Gnorm
library(fs)
library(ggplot2)
# informa o R que o diretório do documento atual 'e o diretorio de trabalho
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Aux_functions.R", encoding="utf-8")
# Gera uma lista com os caminhos dos RDatas do Gnorm
rData_AllGnorm_path_list <- dir_ls("./RDatas_2_100_30", glob = "*.RData")
rData_AllGnorm_path_list
load(rData_AllGnorm_path_list[[1]])
allGnorm = G_norm_mean
for (i in 2:length(rData_AllGnorm_path_list)) {
load(rData_AllGnorm_path_list[[i]])
allGnorm = append(allGnorm, G_norm_mean, after = length(allGnorm))
allGnorm
}
ggplot() + aes(allGnorm)+ geom_histogram(binwidth=0.1, colour="black", fill="#5195B8") +
ggtitle(paste("Distribuicao de G normalizado \n", "Camadas = 2 conexões = 100 nós = 30")) +
#coord_cartesian(ylim = c(0,50)) +
xlab("G") +
labs(x=expression(G["norm"]), y=("Frequência")) +
theme(axis.title.x = element_text(size = 18), axis.title.y = element_text(size = 18))
save(allGnorm, file ="./RDatas_Gnorm_distrib_grupo/rdml_2_100_30_n10")
# Faz a unificação de varias distribuicoes de Gnorm
library(fs)
library(ggplot2)
# informa o R que o diretório do documento atual 'e o diretorio de trabalho
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Aux_functions.R", encoding="utf-8")
# Gera uma lista com os caminhos dos RDatas do Gnorm
rData_AllGnorm_path_list <- dir_ls("./RDatas_2_250_50", glob = "*.RData")
rData_AllGnorm_path_list
load(rData_AllGnorm_path_list[[1]])
allGnorm = G_norm_mean
for (i in 2:length(rData_AllGnorm_path_list)) {
load(rData_AllGnorm_path_list[[i]])
allGnorm = append(allGnorm, G_norm_mean, after = length(allGnorm))
allGnorm
}
ggplot() + aes(allGnorm)+ geom_histogram(binwidth=0.1, colour="black", fill="#5195B8") +
ggtitle(paste("Distribuicao de G normalizado \n", "Camadas = 2 Conexões = 250 Nós = 50", sep = "")) +
#coord_cartesian(ylim = c(0,50)) +
xlab("G") +
labs(x=expression(G["norm"]), y=("Frequência")) +
theme(axis.title.x = element_text(size = 18), axis.title.y = element_text(size = 18))
save(allGnorm, file ="./RDatas_Gnorm_distrib_grupo/rdml_2_250_50_n10")
# Faz a unificação de varias distribuicoes de Gnorm
library(fs)
library(ggplot2)
# informa o R que o diretório do documento atual 'e o diretorio de trabalho
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source("Aux_functions.R", encoding="utf-8")
# Gera uma lista com os caminhos dos RDatas do Gnorm
rData_AllGnorm_path_list <- dir_ls("./RDatas_2_250_75", glob = "*.RData")
rData_AllGnorm_path_list
load(rData_AllGnorm_path_list[[1]])
allGnorm = G_norm_mean
for (i in 2:length(rData_AllGnorm_path_list)) {
load(rData_AllGnorm_path_list[[i]])
allGnorm = append(allGnorm, G_norm_mean, after = length(allGnorm))
allGnorm
}
ggplot() + aes(allGnorm)+ geom_histogram(binwidth=0.1, colour="black", fill="#5195B8") +
ggtitle(paste("Distribuicao de G normalizado \n", "Camadas = 2 Conexões = 250 Nós = 75", sep = "")) +
#coord_cartesian(ylim = c(0,50)) +
xlab("G") +
labs(x=expression(G["norm"]), y=("Frequência")) +
theme(axis.title.x = element_text(size = 18), axis.title.y = element_text(size = 18))
save(allGnorm, file ="./RDatas_Gnorm_distrib_grupo/rdml_2_250_75_n10")
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
source('C:/_Projetos/TF/Analise_em_massa/Automated_Gnorm_distrib.R', encoding = 'UTF-8', echo=TRUE)
